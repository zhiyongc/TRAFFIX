{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Introduction & Process\n",
    "---\n",
    "## 2.1 LOOP-SEA\n",
    "### 2.1.1 Data introduction\n",
    "#### The data download link contains a list of files:\n",
    "* `speed_matrix_2015`: Loop Speed Matrix, which is a pickled file that can be read by pandas or other python packages.\n",
    "* `Loop_Seattle_2015_A.npy`: Loop Adjacency Matrix, which is a numpy matrix to describe the traffic network structure as a graph. \n",
    "* `Loop_Seattle_2015_reachability_free_flow_Xmin.npy`: Loop Free-flow Reachability Matrix during X minites' drive.\n",
    "* `nodes_loop_mp_list.csv`: List of loop detectors' milepost, with the same order of that in the Loop Speed Matrix.\n",
    "\n",
    "#### Loop detectors distribution (<a href=\"./img/cabinet.html\">Cabinets</a>)\n",
    "The data is collected by the **inductive loop detectors deployed on freeways in Seattle area**. The freeways contains I-5, I-405, I-90, and SR-520, shown in the below picture. This dataset contains spatio-temporal speed information of the freeway system. In the picture, each blue icon demonstrates loop detectors at a milepost. The speed information at a milepost is averaged from multiple loop detectors on the mainlanes in a same direction at the specific milepost. The time interval of the dataset is 5-minute. \n",
    "> <img src=\"./img/dataset1-loops.png\" width=\"600\" height=\"400\"></img>\n",
    "\n",
    "#### speed_matrix_2015\n",
    "A demo of the speed_matrix_2015 is shown as the following figure. The horizontal header denotes the milepost and the vertical header indicates the timestamps.\n",
    "><img src='./img/dataset1-sample.png' width=\"800\" height=\"400\"></img>\n",
    "\n",
    "The name of each milepost header contains 11 characters:\n",
    "  * 1 char: 'd' or 'i', i.e. decreasing direction or increasing direction.\n",
    "  * 2-4 chars: route name, e.g. '405' demonstrates the route I-405.\n",
    "  * 5-6 chars: 'es' has no meanings here.\n",
    "  * 7-11 chars: milepost, e.g. '15036' demonstrates the 150.36 milepost.\n",
    "  \n",
    "><img src='./img/dataset1-heatmap.png' width=\"800\" height=\"400\"></img>\n",
    "\n",
    "\n",
    "### 2.1.2 Data Download Link: [Seattle Loop Dataset](https://drive.google.com/drive/folders/1XuK0fgI6lmSUzmToyDdHQy8CPunlm5yr?usp=sharing)\n",
    "\n",
    "#### If you use this dataset in your work, please cite the following reference:\n",
    "###### Reference:\n",
    "* `Cui, Z., Ke, R., & Wang, Y. (2018). Deep Bidirectional and Unidirectional LSTM Recurrent Neural Network for Network-wide Traffic Speed Prediction. arXiv preprint arXiv:1801.02143.`\n",
    "* `Cui, Z., Henrickson, K., Ke, R., & Wang, Y. (2018). High-Order Graph Convolutional Recurrent Neural Network: A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting. arXiv preprint arXiv:1802.07007.`\n",
    "###### BibTex:\n",
    "```\n",
    "@article{cui2018deep,\n",
    "  title={Deep Bidirectional and Unidirectional LSTM Recurrent Neural Network for Network-wide Traffic Speed Prediction},\n",
    "  author={Cui, Zhiyong and Ke, Ruimin and Wang, Yinhai},\n",
    "  journal={arXiv preprint arXiv:1801.02143},\n",
    "  year={2018}\n",
    "} ,\n",
    "@article{cui2018high,\n",
    "  title={High-Order Graph Convolutional Recurrent Neural Network: A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting},\n",
    "  author={Cui, Zhiyong and Henrickson, Kristian and Ke, Ruimin and Wang, Yinhai},\n",
    "  journal={arXiv preprint arXiv:1802.07007},\n",
    "  year={2018}\n",
    "}\n",
    "```\n",
    "#### Note: This dataset should only be used for research.\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "## 2.2 METR-LA and PEMS-BAY\n",
    "### 2.2.1 Data introduction\n",
    "#### These dataset are provided by [Yaguang Li - DCRNN](https://github.com/liyaguang/DCRNN).\n",
    "* **METR-LA**: This traffic dataset contains traffic information collected from loop detectors in the highway of Los Angeles County (Jagadish et al., 2014). We select 207 sensors and collect 4 months of data ranging from Mar 1st 2012 to Jun 30th 2012 for the experiment. The total number of observed traffic data points is 6,519,002.\n",
    "* **PEMS-BAY**: This traffic dataset is collected by California Transportation Agencies (CalTrans) Performance Measurement System (PeMS). We select 325 sensors in the Bay Area and collect 6 months of data ranging from Jan 1st 2017 to May 31th 2017 for the experiment. The total number of observed traffic data points is 16,937,179.\n",
    "\n",
    "#### The data download link contains a list of files:\n",
    "* `metr-la.h5`:METR-LA matrix\n",
    "* `adj_mx.pkl`:METR-LA road network adjacency matrix\n",
    "* `pems-bay.h5`:PEMS-BAY matrix\n",
    "* `adj_mx_bay.pkl`:PEMS-BAY road network adjacency matrix\n",
    "\n",
    "#### Sensors distribution\n",
    "> <img src=\"./img/dataset2-sensors.png\" width=\"800\" height=\"400\"></img>\n",
    "\n",
    "\n",
    "#### (1) METR-LA\n",
    "><img src='./img/dataset2-sample.png' width=\"800\" height=\"400\"></img>\n",
    "><img src='./img/dataset2-heatmap.png' width=\"800\" height=\"400\"></img>\n",
    "\n",
    "#### (2) PEMS-BAY\n",
    "><img src='./img/dataset3-sample.png' width=\"800\" height=\"400\"></img>\n",
    "><img src='./img/dataset3-heatmap.png' width=\"800\" height=\"400\"></img>\n",
    "\n",
    "\n",
    "### 2.2.2 Data Download Link: [METR-LA and PEMS-BAY](https://drive.google.com/drive/folders/10FOTa6HXPqX8Pf5WRoRwcFnW9BrNZEIX)\n",
    "###### Reference:\n",
    "* `Li, Y., Yu, R., Shahabi, C., & Liu, Y. (2017). Diffusion convolutional recurrent neural network: Data-driven traffic forecasting. arXiv preprint arXiv:1707.01926.`\n",
    "###### BibTex:\n",
    "```\n",
    "@inproceedings{li2018dcrnn_traffic,\n",
    "  title={Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting},\n",
    "  author={Li, Yaguang and Yu, Rose and Shahabi, Cyrus and Liu, Yan},\n",
    "  booktitle={International Conference on Learning Representations (ICLR '18)},\n",
    "  year={2018}\n",
    "}\n",
    "```\n",
    "---\n",
    "\n",
    "## 2.3 INRIX-SEA\n",
    "### 2.3.1 Data introduction\n",
    "#### The data download link contains a list of files:\n",
    "* `INRIX_Seattle_Speed_Matrix__2012_v2.pkl`:Speed matrix\n",
    "* `INRIX_Seattle_Adjacency_matrix_2012_v2.npy`:Road network adjacency matrix\n",
    "><img src='./img/dataset4-sample.png' width=\"800\" height=\"400\"></img>\n",
    "><img src='./img/dataset4-heatmap.png' width=\"800\" height=\"400\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Data Process Demo\n",
    "### 2.3.1 Read dataset\n",
    "The downloaded file of the Zenodo version named \"speed_matrix_2015_1mph\" is a Python pickled file that can be read by using [Pandas.read_pickle()](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.read_pickle.html) , and we load the data from the local file and show the top 10 rows as a demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ID</th>\n",
       "      <th>d005es15036</th>\n",
       "      <th>d005es15125</th>\n",
       "      <th>d005es15214</th>\n",
       "      <th>d005es15280</th>\n",
       "      <th>d005es15315</th>\n",
       "      <th>d005es15348</th>\n",
       "      <th>d005es15410</th>\n",
       "      <th>d005es15465</th>\n",
       "      <th>d005es15531</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:00:00</th>\n",
       "      <td>62</td>\n",
       "      <td>64</td>\n",
       "      <td>62</td>\n",
       "      <td>61</td>\n",
       "      <td>63</td>\n",
       "      <td>64</td>\n",
       "      <td>63</td>\n",
       "      <td>65</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:05:00</th>\n",
       "      <td>59</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "      <td>59</td>\n",
       "      <td>62</td>\n",
       "      <td>66</td>\n",
       "      <td>65</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:10:00</th>\n",
       "      <td>62</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>64</td>\n",
       "      <td>62</td>\n",
       "      <td>64</td>\n",
       "      <td>60</td>\n",
       "      <td>65</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:15:00</th>\n",
       "      <td>62</td>\n",
       "      <td>65</td>\n",
       "      <td>67</td>\n",
       "      <td>64</td>\n",
       "      <td>66</td>\n",
       "      <td>65</td>\n",
       "      <td>62</td>\n",
       "      <td>66</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:20:00</th>\n",
       "      <td>62</td>\n",
       "      <td>65</td>\n",
       "      <td>67</td>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>63</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:25:00</th>\n",
       "      <td>63</td>\n",
       "      <td>68</td>\n",
       "      <td>66</td>\n",
       "      <td>63</td>\n",
       "      <td>64</td>\n",
       "      <td>63</td>\n",
       "      <td>62</td>\n",
       "      <td>68</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:30:00</th>\n",
       "      <td>62</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>64</td>\n",
       "      <td>63</td>\n",
       "      <td>68</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:35:00</th>\n",
       "      <td>64</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>63</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>61</td>\n",
       "      <td>65</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:40:00</th>\n",
       "      <td>63</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>65</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>60</td>\n",
       "      <td>65</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:45:00</th>\n",
       "      <td>62</td>\n",
       "      <td>66</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>64</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>67</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ID                   d005es15036  d005es15125  d005es15214  d005es15280  \\\n",
       "stamp                                                                     \n",
       "2015-01-01 00:00:00           62           64           62           61   \n",
       "2015-01-01 00:05:00           59           65           65           66   \n",
       "2015-01-01 00:10:00           62           65           65           64   \n",
       "2015-01-01 00:15:00           62           65           67           64   \n",
       "2015-01-01 00:20:00           62           65           67           65   \n",
       "2015-01-01 00:25:00           63           68           66           63   \n",
       "2015-01-01 00:30:00           62           65           65           62   \n",
       "2015-01-01 00:35:00           64           67           67           63   \n",
       "2015-01-01 00:40:00           63           67           67           65   \n",
       "2015-01-01 00:45:00           62           66           65           65   \n",
       "\n",
       "ID                   d005es15315  d005es15348  d005es15410  d005es15465  \\\n",
       "stamp                                                                     \n",
       "2015-01-01 00:00:00           63           64           63           65   \n",
       "2015-01-01 00:05:00           59           62           66           65   \n",
       "2015-01-01 00:10:00           62           64           60           65   \n",
       "2015-01-01 00:15:00           66           65           62           66   \n",
       "2015-01-01 00:20:00           66           65           65           63   \n",
       "2015-01-01 00:25:00           64           63           62           68   \n",
       "2015-01-01 00:30:00           62           64           63           68   \n",
       "2015-01-01 00:35:00           65           65           61           65   \n",
       "2015-01-01 00:40:00           63           63           60           65   \n",
       "2015-01-01 00:45:00           64           63           63           67   \n",
       "\n",
       "ID                   d005es15531  \n",
       "stamp                             \n",
       "2015-01-01 00:00:00           63  \n",
       "2015-01-01 00:05:00           61  \n",
       "2015-01-01 00:10:00           64  \n",
       "2015-01-01 00:15:00           62  \n",
       "2015-01-01 00:20:00           62  \n",
       "2015-01-01 00:25:00           61  \n",
       "2015-01-01 00:30:00           61  \n",
       "2015-01-01 00:35:00           61  \n",
       "2015-01-01 00:40:00           62  \n",
       "2015-01-01 00:45:00           63  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset1 = pd.read_pickle('./LOOP-SEA/speed_matrix_2015_1mph')\n",
    "dataset2 = pd.read_hdf('./METR-LA/metr-la.h5')\n",
    "dataset3 = pd.read_hdf('./PEMS-BAY/pems-bay.h5')\n",
    "dataset4 = pd.read_pickle('./INRIX-SEA/INRIX_Seattle_Speed_Matrix__2012_v2.pkl')\n",
    "dataset1.iloc[:,:9].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is formatted into a matrix, whose horizontal and vertical axes display loop detector names and timestamps. \n",
    "\n",
    "The **temporal dimension** of the dataset is 105120 and the **spatial dimension** is 323. \n",
    "\n",
    "Here, the temporal dimension is 105120 = 365 (days) * 24 (hours) * 12 (5-minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset1(Loop-SEA): (105120, 323)\n",
      "dataset2(METR-LA): (34272, 207)\n",
      "dataset3(PEMS-BAY): (52116, 325)\n",
      "dataset4(INRIX_SEA): (105312, 745)\n"
     ]
    }
   ],
   "source": [
    "print('dataset1(Loop-SEA):', dataset1.shape)\n",
    "print('dataset2(METR-LA):', dataset2.shape)\n",
    "print('dataset3(PEMS-BAY):', dataset3.shape)\n",
    "print('dataset4(INRIX_SEA):', dataset4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Format data into training, validation, and testing sets\n",
    "We first convert the data matrix into three sub-sets, i.e the training, validation, and testing sets. \n",
    "\n",
    "Normally, the **Training : Validation : Testing** ratio can be **7:2:1** or **6:2:2**.  \n",
    "\n",
    "Some meta parameters are listed as below:\n",
    "\n",
    "> <img src=\"./img/data-process.png\" width=\"800\" height=\"400\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import utils\n",
    "\n",
    "class PrepareDataset():    \n",
    "    \"\"\" Prepare training and testing datasets and dataloaders.\n",
    "    \n",
    "    Convert speed/volume/occupancy matrix to training, validation, and testing sets. \n",
    "    The vertical axis of speed_matrix is the time axis and the horizontal axis \n",
    "    is the spatial axis.\n",
    "    \n",
    "    Args:\n",
    "        dataset: a Matrix containing spatial-temporal speed data for a network\n",
    "        seq_len: length of input sequence\n",
    "        pred_len: length of predicted sequence\n",
    "    Returns:\n",
    "        Training dataloader\n",
    "        Testing dataloader\n",
    "    \"\"\"\n",
    "    def __init__(self,batch_size=40,seq_len=10,pred_len=1,train_propotion=0.7,valid_propotion=0.2):\n",
    "        self.BATCH_SIZE = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.train_propotion = train_propotion\n",
    "        self.valid_propotion = valid_propotion\n",
    "        \n",
    "    def input_output_extract(self,dataset):\n",
    "        np.random.seed(1024)\n",
    "        time_len = dataset.shape[0]\n",
    "        dataset =  dataset / dataset.max().max()\n",
    "\n",
    "        data_sequences, data_labels = [], []\n",
    "        for i in range(time_len - self.seq_len - self.pred_len):\n",
    "            data_sequences.append(dataset.iloc[i:i+self.seq_len].values)\n",
    "            data_labels.append(dataset.iloc[i+self.seq_len:i+self.seq_len+self.pred_len].values)\n",
    "        data_sequences, data_labels = np.asarray(data_sequences), np.asarray(data_labels)\n",
    "        return data_sequences, data_labels\n",
    "    \n",
    "    def train_test_split(self,data_sequences,data_labels):\n",
    "        # shuffle and split the dataset to training and testing datasets\n",
    "        sample_size = data_sequences.shape[0]\n",
    "        index = np.arange(sample_size, dtype = int)\n",
    "        np.random.shuffle(index)\n",
    "\n",
    "        train_index = int(np.floor(sample_size * self.train_propotion))\n",
    "        valid_index = int(np.floor(sample_size * ( self.train_propotion + self.valid_propotion)))\n",
    "\n",
    "        train_data, train_label = data_sequences[:train_index], data_labels[:train_index]\n",
    "        valid_data, valid_label = data_sequences[train_index:valid_index], data_labels[train_index:valid_index]\n",
    "        test_data, test_label = data_sequences[valid_index:], data_labels[valid_index:]\n",
    "\n",
    "        train_data, train_label = torch.Tensor(train_data), torch.Tensor(train_label)\n",
    "        valid_data, valid_label = torch.Tensor(valid_data), torch.Tensor(valid_label)\n",
    "        test_data, test_label = torch.Tensor(test_data), torch.Tensor(test_label)\n",
    "\n",
    "        train_dataset = utils.data.TensorDataset(train_data, train_label)\n",
    "        valid_dataset = utils.data.TensorDataset(valid_data, valid_label)\n",
    "        test_dataset = utils.data.TensorDataset(test_data, test_label)\n",
    "\n",
    "        train_dataloader = utils.data.DataLoader(train_dataset, batch_size = self.BATCH_SIZE, shuffle=True, drop_last = True)\n",
    "        valid_dataloader = utils.data.DataLoader(valid_dataset, batch_size = self.BATCH_SIZE, shuffle=True, drop_last = True)\n",
    "        test_dataloader = utils.data.DataLoader(test_dataset, batch_size = self.BATCH_SIZE, shuffle=True, drop_last = True)\n",
    "\n",
    "        return train_dataloader, valid_dataloader, test_dataloader\n",
    "    \n",
    "    def fit(self, dataset):\n",
    "        data_sequences, data_labels = self.input_output_extract(dataset)\n",
    "        train_dataloader, valid_dataloader, test_dataloader = self.train_test_split(data_sequences,data_labels)\n",
    "        return train_dataloader, valid_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdt = PrepareDataset(batch_size = 40, seq_len = 10, pred_len = 1, train_propotion = 0.7, valid_propotion = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = pdt.input_output_extract(dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105109, 10, 323) (105109, 1, 323)\n"
     ]
    }
   ],
   "source": [
    "print(sequences.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = pdt.fit(dataset1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
