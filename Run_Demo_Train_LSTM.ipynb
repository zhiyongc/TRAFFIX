{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Run_Demo_Train_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhiyongc/TRAFFIX/blob/master/Run_Demo_Train_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6EWw3zgYxfF",
        "colab_type": "code",
        "outputId": "4329abcc-6105-47a3-8ec7-8038fae6b0e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as utils\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Parameter, ParameterList\n",
        "print('PyTorch Version:', torch.__version__)\n",
        "print('Using GPU:', torch.cuda.is_available())\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch Version: 1.3.1\n",
            "Using GPU: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8V5wprDHICh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dtype = torch.float32\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "torch.cuda.set_device(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXkc-CKpYORx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import io\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_ZXwG_RY1J3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df = pd.read_csv('https://github.com/zhiyongc/TRAFFIX/blob/master/2015_third_week.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgOcPPE_Dkul",
        "colab_type": "code",
        "outputId": "9a9f10d8-d8e6-42de-acd4-cac4617d50e2",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d9967f07-0ab6-44ae-9f8d-ce6fa2dd6c74\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-d9967f07-0ab6-44ae-9f8d-ce6fa2dd6c74\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving 2015_third_week.csv to 2015_third_week.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iLWj1-fEB2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "speed_matrix = pd.read_csv(io.StringIO(uploaded['2015_third_week.csv'].decode('utf-8')),index_col=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl_7FoyqFcdT",
        "colab_type": "code",
        "outputId": "2316d70f-d2a1-465b-d804-4dfc206a35e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        }
      },
      "source": [
        "speed_matrix"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>d005es16583</th>\n",
              "      <th>d005es16640</th>\n",
              "      <th>d005es16661</th>\n",
              "      <th>d005es16701</th>\n",
              "      <th>d005es16732</th>\n",
              "      <th>d005es16756</th>\n",
              "      <th>d005es16802</th>\n",
              "      <th>d005es16831</th>\n",
              "      <th>d005es16885</th>\n",
              "      <th>d005es16919</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2015-01-15 00:00:00</th>\n",
              "      <td>66</td>\n",
              "      <td>64</td>\n",
              "      <td>67</td>\n",
              "      <td>57</td>\n",
              "      <td>51</td>\n",
              "      <td>67</td>\n",
              "      <td>57</td>\n",
              "      <td>64</td>\n",
              "      <td>56</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-15 00:05:00</th>\n",
              "      <td>65</td>\n",
              "      <td>64</td>\n",
              "      <td>67</td>\n",
              "      <td>62</td>\n",
              "      <td>54</td>\n",
              "      <td>66</td>\n",
              "      <td>61</td>\n",
              "      <td>61</td>\n",
              "      <td>64</td>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-15 00:10:00</th>\n",
              "      <td>65</td>\n",
              "      <td>63</td>\n",
              "      <td>66</td>\n",
              "      <td>62</td>\n",
              "      <td>61</td>\n",
              "      <td>66</td>\n",
              "      <td>59</td>\n",
              "      <td>63</td>\n",
              "      <td>63</td>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-15 00:15:00</th>\n",
              "      <td>67</td>\n",
              "      <td>63</td>\n",
              "      <td>68</td>\n",
              "      <td>61</td>\n",
              "      <td>62</td>\n",
              "      <td>71</td>\n",
              "      <td>61</td>\n",
              "      <td>60</td>\n",
              "      <td>63</td>\n",
              "      <td>65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-15 00:20:00</th>\n",
              "      <td>63</td>\n",
              "      <td>59</td>\n",
              "      <td>65</td>\n",
              "      <td>59</td>\n",
              "      <td>58</td>\n",
              "      <td>65</td>\n",
              "      <td>58</td>\n",
              "      <td>59</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-21 23:35:00</th>\n",
              "      <td>66</td>\n",
              "      <td>64</td>\n",
              "      <td>65</td>\n",
              "      <td>54</td>\n",
              "      <td>61</td>\n",
              "      <td>64</td>\n",
              "      <td>67</td>\n",
              "      <td>65</td>\n",
              "      <td>62</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-21 23:40:00</th>\n",
              "      <td>65</td>\n",
              "      <td>66</td>\n",
              "      <td>65</td>\n",
              "      <td>59</td>\n",
              "      <td>61</td>\n",
              "      <td>65</td>\n",
              "      <td>65</td>\n",
              "      <td>63</td>\n",
              "      <td>60</td>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-21 23:45:00</th>\n",
              "      <td>66</td>\n",
              "      <td>65</td>\n",
              "      <td>65</td>\n",
              "      <td>59</td>\n",
              "      <td>59</td>\n",
              "      <td>63</td>\n",
              "      <td>65</td>\n",
              "      <td>62</td>\n",
              "      <td>62</td>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-21 23:50:00</th>\n",
              "      <td>66</td>\n",
              "      <td>63</td>\n",
              "      <td>66</td>\n",
              "      <td>60</td>\n",
              "      <td>56</td>\n",
              "      <td>66</td>\n",
              "      <td>67</td>\n",
              "      <td>68</td>\n",
              "      <td>65</td>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-21 23:55:00</th>\n",
              "      <td>62</td>\n",
              "      <td>54</td>\n",
              "      <td>66</td>\n",
              "      <td>62</td>\n",
              "      <td>57</td>\n",
              "      <td>62</td>\n",
              "      <td>57</td>\n",
              "      <td>63</td>\n",
              "      <td>64</td>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2016 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                     d005es16583  d005es16640  ...  d005es16885  d005es16919\n",
              "stamp                                          ...                          \n",
              "2015-01-15 00:00:00           66           64  ...           56           63\n",
              "2015-01-15 00:05:00           65           64  ...           64           67\n",
              "2015-01-15 00:10:00           65           63  ...           63           62\n",
              "2015-01-15 00:15:00           67           63  ...           63           65\n",
              "2015-01-15 00:20:00           63           59  ...           64           64\n",
              "...                          ...          ...  ...          ...          ...\n",
              "2015-01-21 23:35:00           66           64  ...           62           66\n",
              "2015-01-21 23:40:00           65           66  ...           60           67\n",
              "2015-01-21 23:45:00           66           65  ...           62           67\n",
              "2015-01-21 23:50:00           66           63  ...           65           67\n",
              "2015-01-21 23:55:00           62           54  ...           64           67\n",
              "\n",
              "[2016 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeMTmgnpEjBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PrepareDataset(speed_matrix, BATCH_SIZE = 40, seq_len = 10, pred_len = 1, train_propotion = 0.7, valid_propotion = 0.2):\n",
        "    \"\"\" Prepare training and testing datasets and dataloaders.\n",
        "    \n",
        "    Convert speed/volume/occupancy matrix to training and testing dataset. \n",
        "    The vertical axis of speed_matrix is the time axis and the horizontal axis \n",
        "    is the spatial axis.\n",
        "    \n",
        "    Args:\n",
        "        speed_matrix: a Matrix containing spatial-temporal speed data for a network\n",
        "        seq_len: length of input sequence\n",
        "        pred_len: length of predicted sequence\n",
        "    Returns:\n",
        "        Training dataloader\n",
        "        Testing dataloader\n",
        "    \"\"\"\n",
        "    time_len = speed_matrix.shape[0]\n",
        "    \n",
        "    max_speed = speed_matrix.max().max()\n",
        "    speed_matrix =  speed_matrix / max_speed\n",
        "    \n",
        "    speed_sequences, speed_labels = [], []\n",
        "    for i in range(time_len - seq_len - pred_len):\n",
        "        speed_sequences.append(speed_matrix.iloc[i:i+seq_len].values)\n",
        "        speed_labels.append(speed_matrix.iloc[i+seq_len:i+seq_len+pred_len].values)\n",
        "    speed_sequences, speed_labels = np.asarray(speed_sequences), np.asarray(speed_labels)\n",
        "    \n",
        "    # shuffle and split the dataset to training and testing datasets\n",
        "    sample_size = speed_sequences.shape[0]\n",
        "    index = np.arange(sample_size, dtype = int)\n",
        "    np.random.shuffle(index)\n",
        "    \n",
        "    train_index = int(np.floor(sample_size * train_propotion))\n",
        "    valid_index = int(np.floor(sample_size * ( train_propotion + valid_propotion)))\n",
        "    \n",
        "    train_data, train_label = speed_sequences[:train_index], speed_labels[:train_index]\n",
        "    valid_data, valid_label = speed_sequences[train_index:valid_index], speed_labels[train_index:valid_index]\n",
        "    test_data, test_label = speed_sequences[valid_index:], speed_labels[valid_index:]\n",
        "    \n",
        "    train_data, train_label = torch.Tensor(train_data), torch.Tensor(train_label)\n",
        "    valid_data, valid_label = torch.Tensor(valid_data), torch.Tensor(valid_label)\n",
        "    test_data, test_label = torch.Tensor(test_data), torch.Tensor(test_label)\n",
        "    \n",
        "    train_dataset = utils.TensorDataset(train_data, train_label)\n",
        "    valid_dataset = utils.TensorDataset(valid_data, valid_label)\n",
        "    test_dataset = utils.TensorDataset(test_data, test_label)\n",
        "    \n",
        "    train_dataloader = utils.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True, drop_last = True)\n",
        "    valid_dataloader = utils.DataLoader(valid_dataset, batch_size = BATCH_SIZE, shuffle=True, drop_last = True)\n",
        "    test_dataloader = utils.DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle=True, drop_last = True)\n",
        "    \n",
        "    return train_dataloader, valid_dataloader, test_dataloader, max_speed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz0XXi2bErp3",
        "colab_type": "code",
        "outputId": "7fc44385-8697-4fdc-d1a2-02a4f2f6d02d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "starttime = time.time()\n",
        "train_dataloader, valid_dataloader, test_dataloader, max_speed = PrepareDataset(speed_matrix)\n",
        "print( time.time() - starttime )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.33252739906311035\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eDLcgRCEtXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, n, output_last = True):\n",
        "        \"\"\"\n",
        "        cell_size is the size of cell_state.\n",
        "        hidden_size is the size of hidden_state, or say the output_state of each step\n",
        "        \"\"\"\n",
        "        super(LSTM, self).__init__()\n",
        "        \n",
        "        self.n = n\n",
        "        self.fl = nn.Linear(2 * n, n)\n",
        "        self.il = nn.Linear(2 * n, n)\n",
        "        self.ol = nn.Linear(2 * n, n)\n",
        "        self.Cl = nn.Linear(2 * n, n)\n",
        "        \n",
        "        self.output_last = output_last\n",
        "        \n",
        "    def step(self, input, Hidden_State, Cell_State):\n",
        "        combined = torch.cat((input, Hidden_State), 1)\n",
        "        f = F.sigmoid(self.fl(combined))\n",
        "        i = F.sigmoid(self.il(combined))\n",
        "        o = F.sigmoid(self.ol(combined))\n",
        "        C = F.tanh(self.Cl(combined))\n",
        "        Cell_State = f * Cell_State + i * C\n",
        "        Hidden_State = o * F.tanh(Cell_State)\n",
        "        \n",
        "        return Hidden_State, Cell_State\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        batch_size = inputs.size(0)\n",
        "        time_step = inputs.size(1)\n",
        "        spatial_size = inputs.shape[2]\n",
        "        Hidden_State, Cell_State = self.initHidden(batch_size, spatial_size)\n",
        "        \n",
        "        if self.output_last:\n",
        "            for i in range(time_step):\n",
        "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:]), Hidden_State, Cell_State)  \n",
        "            return Hidden_State\n",
        "        else:\n",
        "            outputs = None\n",
        "            for i in range(time_step):\n",
        "                Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:]), Hidden_State, Cell_State)  \n",
        "                if outputs is None:\n",
        "                    outputs = Hidden_State.unsqueeze(1)\n",
        "                else:\n",
        "                    outputs = torch.cat((outputs, Hidden_State.unsqueeze(1)), 1)\n",
        "            return outputs\n",
        "    \n",
        "    def initHidden(self, batch_size, spatial_size):\n",
        "        Hidden_State = torch.zeros((batch_size, spatial_size), dtype = dtype, device = device)\n",
        "        Cell_State = torch.zeros((batch_size, spatial_size), dtype = dtype, device = device)\n",
        "        return Hidden_State, Cell_State\n",
        "       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aC6YvbrNGIFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TrainModel(model, train_dataloader, valid_dataloader, learning_rate = 1e-5, optm = 'Adam', num_epochs = 300, patience = 5, min_delta = 0.00001):\n",
        "    \n",
        "    inputs, labels = next(iter(train_dataloader))\n",
        "    [batch_size, step_size, fea_size] = inputs.size()\n",
        "    input_dim = fea_size\n",
        "    hidden_dim = fea_size\n",
        "    output_dim = fea_size\n",
        "    \n",
        "    model.cuda()\n",
        "    \n",
        "    loss_MSE = torch.nn.MSELoss()\n",
        "    loss_L1 = torch.nn.L1Loss()\n",
        "\n",
        "    learning_rate = learning_rate\n",
        "    if optm == 'Adam':\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "    elif optm == 'Adadelta':\n",
        "        optimizer = torch.optim.Adadelta(model.parameters(), lr = learning_rate)\n",
        "    elif optm == 'RMSprop':\n",
        "        optimizer = torch.optim.RMSprop(model.parameters(), lr = learning_rate)\n",
        "        \n",
        "    use_gpu = torch.cuda.is_available()\n",
        "    \n",
        "    losses_train = []\n",
        "    losses_valid = []\n",
        "    losses_epochs_train = []\n",
        "    losses_epochs_valid = []\n",
        "    \n",
        "    train_time_list = []\n",
        "    valid_time_list = []\n",
        "    \n",
        "    cur_time = time.time()\n",
        "    pre_time = time.time()\n",
        "    \n",
        "    # Variables for Early Stopping\n",
        "    is_best_model = 0\n",
        "    patient_epoch = 0\n",
        "    \n",
        "    sub_epoch = 1\n",
        "    \n",
        "    for epoch in range(0, num_epochs):\n",
        "\n",
        "        losses_epoch_train = []\n",
        "        losses_epoch_valid = []\n",
        "\n",
        "        train_start = time.time()\n",
        "        \n",
        "        for data in train_dataloader:\n",
        "            inputs, labels = data\n",
        "\n",
        "            if inputs.shape[0] != batch_size:\n",
        "                continue\n",
        "\n",
        "            if use_gpu:\n",
        "                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
        "            else: \n",
        "                inputs, labels = Variable(inputs), Variable(labels)\n",
        "                \n",
        "            model.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            if len(labels[labels==0]): \n",
        "                label_mask = torch.ones_like(labels).cuda()\n",
        "                label_mask = label_mask * labels\n",
        "                label_mask[label_mask!=0] = 1\n",
        "                loss_train = loss_MSE(outputs * label_mask, torch.squeeze(labels)) \n",
        "            else:\n",
        "                loss_train = loss_MSE(outputs, torch.squeeze(labels)) \n",
        "\n",
        "            losses_epoch_train.append(loss_train.item())\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            loss_train.backward()\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "        train_end = time.time()\n",
        "        \n",
        "        # validation   \n",
        "        valid_start = time.time()\n",
        "        \n",
        "        for data in valid_dataloader:\n",
        "            \n",
        "            inputs_val, labels_val = data\n",
        "            \n",
        "            if use_gpu:\n",
        "                inputs_val, labels_val = Variable(inputs_val.cuda()), Variable(labels_val.cuda())\n",
        "            else: \n",
        "                inputs_val, labels_val = Variable(inputs_val), Variable(labels_val)\n",
        "\n",
        "            outputs_val= model(inputs_val)\n",
        "            \n",
        "            if len(labels_val[labels_val==0]): \n",
        "                labels_val_mask = torch.ones_like(labels_val).cuda()\n",
        "                labels_val_mask = labels_val_mask * labels_val\n",
        "                labels_val_mask[labels_val_mask!=0] = 1\n",
        "\n",
        "                loss_valid = loss_MSE(outputs_val * labels_val_mask, torch.squeeze(labels_val))\n",
        "            else:\n",
        "                loss_valid = loss_MSE(outputs_val , torch.squeeze(labels_val))\n",
        "            \n",
        "            losses_epoch_valid.append(loss_valid.item())\n",
        "        \n",
        "        valid_end = time.time()\n",
        "        \n",
        "        avg_losses_epoch_train = sum(losses_epoch_train) / float(len(losses_epoch_train))\n",
        "        avg_losses_epoch_valid = sum(losses_epoch_valid) / float(len(losses_epoch_valid))\n",
        "        losses_epochs_train.append(avg_losses_epoch_train)\n",
        "        losses_epochs_valid.append(avg_losses_epoch_valid)\n",
        "        \n",
        "\n",
        "        # Early Stopping\n",
        "        if epoch == 0:\n",
        "            is_best_model = 1\n",
        "            best_model = copy.deepcopy(model)\n",
        "            min_loss_epoch_valid = 10000.0\n",
        "            if avg_losses_epoch_valid < min_loss_epoch_valid:\n",
        "                min_loss_epoch_valid = avg_losses_epoch_valid\n",
        "                \n",
        "#             sub_epoch += 1\n",
        "        else:\n",
        "            if min_loss_epoch_valid - avg_losses_epoch_valid > min_delta:\n",
        "                is_best_model = 1\n",
        "                best_model = model\n",
        "                min_loss_epoch_valid = avg_losses_epoch_valid \n",
        "                patient_epoch = 0\n",
        "            else:\n",
        "                is_best_model = 0\n",
        "                patient_epoch += 1\n",
        "                if patient_epoch >= patience:\n",
        "                    print('Early Stopped at Epoch:', epoch)\n",
        "                    break\n",
        "                    \n",
        "        if (epoch >= 5 and (patient_epoch == 4 or sub_epoch % 50 == 0)) and learning_rate > 1e-5:\n",
        "            learning_rate = learning_rate / 10\n",
        "            if optm == 'Adam':\n",
        "                optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "            elif optm == 'Adadelta':\n",
        "                optimizer = torch.optim.Adadelta(model.parameters(), lr = learning_rate)\n",
        "            elif optm == 'RMSprop':\n",
        "                optimizer = torch.optim.RMSprop(model.parameters(), lr = learning_rate)\n",
        "            sub_epoch = 1\n",
        "        else:\n",
        "            sub_epoch += 1\n",
        "                        \n",
        "        \n",
        "        # Print training parameters\n",
        "        cur_time = time.time()\n",
        "        train_time = np.around([train_end - train_start] , decimals=2)\n",
        "        train_time_list.append(train_time)\n",
        "        valid_time = np.around([valid_end - valid_start] , decimals=2)\n",
        "        valid_time_list.append(valid_time)\n",
        "        \n",
        "        print('Epoch: {}, train_loss: {}, valid_loss: {}, lr: {}, train_time: {}, valid_time: {}, best model: {}'.format( \\\n",
        "                    epoch, \\\n",
        "                    np.around(avg_losses_epoch_train, decimals=8),\\\n",
        "                    np.around(avg_losses_epoch_valid, decimals=8),\\\n",
        "                    learning_rate,\\\n",
        "                    np.around([train_end - train_start] , decimals=2),\\\n",
        "                    np.around([valid_end - valid_start] , decimals=2),\\\n",
        "                    is_best_model) )\n",
        "        pre_time = cur_time\n",
        "    \n",
        "    train_time_avg = np.mean(np.array(train_time_list))\n",
        "    valid_time_avg = np.mean(np.array(valid_time_list))\n",
        "    \n",
        "    return best_model, [losses_train, losses_valid, losses_epochs_train, losses_epochs_valid, train_time_avg, valid_time_avg]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0wOXkNEGMz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TestModel(model, test_dataloader, max_speed):\n",
        "    \n",
        "    inputs, labels = next(iter(test_dataloader))\n",
        "    [batch_size, step_size, fea_size] = inputs.size()\n",
        "\n",
        "    cur_time = time.time()\n",
        "    pre_time = time.time()\n",
        "    \n",
        "    use_gpu = torch.cuda.is_available()\n",
        "    \n",
        "    loss_MSE = torch.nn.MSELoss()\n",
        "    loss_L1 = torch.nn.L1Loss()\n",
        "    \n",
        "    tested_batch = 0\n",
        "    \n",
        "    losses_mse = []\n",
        "    losses_l1 = [] \n",
        "\n",
        "    output_list = []\n",
        "    label_list = []\n",
        "    \n",
        "    losses_l1_allsteps = None\n",
        "    \n",
        "    for data in test_dataloader:\n",
        "        inputs, labels = data\n",
        "        \n",
        "        if inputs.shape[0] != batch_size:\n",
        "            continue\n",
        "    \n",
        "        if use_gpu:\n",
        "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
        "        else: \n",
        "            inputs, labels = Variable(inputs), Variable(labels)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        \n",
        "        loss_mse = loss_MSE(outputs, torch.squeeze(labels))\n",
        "        loss_l1 = loss_L1(outputs, torch.squeeze(labels))\n",
        "        \n",
        "        losses_mse.append(loss_mse.cpu().data.numpy())\n",
        "        losses_l1.append(loss_l1.cpu().data.numpy())\n",
        "        \n",
        "        output_list.append(torch.squeeze(outputs).cpu().data.numpy())\n",
        "        label_list.append(torch.squeeze(labels).cpu().data.numpy())\n",
        "        \n",
        "        tested_batch += 1\n",
        "    \n",
        "        if tested_batch % 1000 == 0:\n",
        "            cur_time = time.time()\n",
        "            print('Tested #: {}, loss_l1: {}, loss_mse: {}, time: {}'.format( \\\n",
        "                  tested_batch * batch_size, \\\n",
        "                  np.around([loss_l1.data[0]], decimals=8), \\\n",
        "                  np.around([loss_mse.data[0]], decimals=8), \\\n",
        "                  np.around([cur_time - pre_time], decimals=8) ) )\n",
        "            pre_time = cur_time\n",
        "#     print(losses_l1)\n",
        "    losses_l1 = np.array(losses_l1)\n",
        "    losses_mse = np.array(losses_mse)\n",
        "    output_list = np.array(output_list)\n",
        "    label_list = np.array(label_list)\n",
        "    \n",
        "    non_zero_index = np.nonzero(label_list)\n",
        "    MAE = np.mean(np.absolute(output_list[non_zero_index] - label_list[non_zero_index])) * max_speed\n",
        "    RMSE = np.sqrt(np.mean(np.square(output_list[non_zero_index]* max_speed - label_list[non_zero_index]* max_speed)))\n",
        "    MAPE = np.mean(np.absolute(output_list[non_zero_index] - label_list[non_zero_index])/label_list[non_zero_index]) * 100         \n",
        "    MAE = np.around(MAE, decimals=3)\n",
        "    RMSE = np.around(RMSE, decimals=3)\n",
        "    MAPE = np.around(MAPE, decimals=3)\n",
        "    print('Tested: MAE: {}, RMSE : {}, MAPE : {} %, '.format( MAE, RMSE, MAPE))\n",
        "    return [losses_l1, losses_mse, None, MAE, RMSE, MAPE, losses_l1_allsteps]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYb68U9MGrFN",
        "colab_type": "code",
        "outputId": "70dfdeaa-4763-46da-b71a-e339e9742d73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "inputs, labels = next(iter(train_dataloader))\n",
        "[batch_size, step_size, fea_size] = inputs.size()\n",
        "[batch_size, step_size, fea_size]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[40, 10, 10]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bjfz5uEgGleG",
        "colab_type": "code",
        "outputId": "fdf03005-4fca-4fb8-94f8-1d87f2968d90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "lstm = LSTM(fea_size, output_last = True).cuda()\n",
        "lstm, lstm_loss = TrainModel(lstm, train_dataloader, valid_dataloader, learning_rate = 1e-1, optm = 'Adam', num_epochs = 300, patience = 5, min_delta = 0.00001)\n",
        "lstm_test = TestModel(lstm, test_dataloader, max_speed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, train_loss: 0.08013319, valid_loss: 0.01390901, lr: 0.1, train_time: [0.34], valid_time: [0.04], best model: 1\n",
            "Epoch: 1, train_loss: 0.01005609, valid_loss: 0.00788602, lr: 0.1, train_time: [0.32], valid_time: [0.04], best model: 1\n",
            "Epoch: 2, train_loss: 0.0064909, valid_loss: 0.00626857, lr: 0.1, train_time: [0.32], valid_time: [0.04], best model: 1\n",
            "Epoch: 3, train_loss: 0.00554803, valid_loss: 0.00603422, lr: 0.1, train_time: [0.31], valid_time: [0.04], best model: 1\n",
            "Epoch: 4, train_loss: 0.00504223, valid_loss: 0.00530364, lr: 0.1, train_time: [0.31], valid_time: [0.04], best model: 1\n",
            "Epoch: 5, train_loss: 0.00482535, valid_loss: 0.00551325, lr: 0.1, train_time: [0.33], valid_time: [0.03], best model: 0\n",
            "Epoch: 6, train_loss: 0.0046772, valid_loss: 0.0052831, lr: 0.1, train_time: [0.32], valid_time: [0.04], best model: 1\n",
            "Epoch: 7, train_loss: 0.00442752, valid_loss: 0.0049296, lr: 0.1, train_time: [0.32], valid_time: [0.04], best model: 1\n",
            "Epoch: 8, train_loss: 0.00420118, valid_loss: 0.00463719, lr: 0.1, train_time: [0.31], valid_time: [0.04], best model: 1\n",
            "Epoch: 9, train_loss: 0.00423489, valid_loss: 0.00515029, lr: 0.1, train_time: [0.33], valid_time: [0.03], best model: 0\n",
            "Epoch: 10, train_loss: 0.00412607, valid_loss: 0.004692, lr: 0.1, train_time: [0.32], valid_time: [0.04], best model: 0\n",
            "Epoch: 11, train_loss: 0.00392564, valid_loss: 0.00478334, lr: 0.1, train_time: [0.31], valid_time: [0.04], best model: 0\n",
            "Epoch: 12, train_loss: 0.0038781, valid_loss: 0.00426944, lr: 0.1, train_time: [0.31], valid_time: [0.04], best model: 1\n",
            "Epoch: 13, train_loss: 0.00369409, valid_loss: 0.00429233, lr: 0.1, train_time: [0.33], valid_time: [0.03], best model: 0\n",
            "Epoch: 14, train_loss: 0.00360386, valid_loss: 0.00442761, lr: 0.1, train_time: [0.31], valid_time: [0.04], best model: 0\n",
            "Epoch: 15, train_loss: 0.00368365, valid_loss: 0.00448231, lr: 0.1, train_time: [0.32], valid_time: [0.03], best model: 0\n",
            "Epoch: 16, train_loss: 0.00359752, valid_loss: 0.00407536, lr: 0.1, train_time: [0.32], valid_time: [0.04], best model: 1\n",
            "Epoch: 17, train_loss: 0.00355736, valid_loss: 0.00414104, lr: 0.1, train_time: [0.31], valid_time: [0.04], best model: 0\n",
            "Epoch: 18, train_loss: 0.00353434, valid_loss: 0.00410876, lr: 0.1, train_time: [0.31], valid_time: [0.03], best model: 0\n",
            "Epoch: 19, train_loss: 0.00348514, valid_loss: 0.00412973, lr: 0.1, train_time: [0.32], valid_time: [0.03], best model: 0\n",
            "Epoch: 20, train_loss: 0.00344371, valid_loss: 0.00393187, lr: 0.1, train_time: [0.31], valid_time: [0.04], best model: 1\n",
            "Epoch: 21, train_loss: 0.00345321, valid_loss: 0.00395107, lr: 0.1, train_time: [0.31], valid_time: [0.03], best model: 0\n",
            "Epoch: 22, train_loss: 0.0033567, valid_loss: 0.00402558, lr: 0.1, train_time: [0.32], valid_time: [0.03], best model: 0\n",
            "Epoch: 23, train_loss: 0.00332386, valid_loss: 0.00416386, lr: 0.1, train_time: [0.31], valid_time: [0.04], best model: 0\n",
            "Epoch: 24, train_loss: 0.00325564, valid_loss: 0.00399363, lr: 0.01, train_time: [0.32], valid_time: [0.04], best model: 0\n",
            "Epoch: 25, train_loss: 0.00311875, valid_loss: 0.00377061, lr: 0.01, train_time: [0.32], valid_time: [0.04], best model: 1\n",
            "Epoch: 26, train_loss: 0.00307184, valid_loss: 0.00368114, lr: 0.01, train_time: [0.32], valid_time: [0.04], best model: 1\n",
            "Epoch: 27, train_loss: 0.0030367, valid_loss: 0.00368138, lr: 0.01, train_time: [0.31], valid_time: [0.04], best model: 0\n",
            "Epoch: 28, train_loss: 0.00285903, valid_loss: 0.00325577, lr: 0.01, train_time: [0.32], valid_time: [0.04], best model: 1\n",
            "Epoch: 29, train_loss: 0.00265757, valid_loss: 0.00320845, lr: 0.01, train_time: [0.31], valid_time: [0.04], best model: 1\n",
            "Epoch: 30, train_loss: 0.00264754, valid_loss: 0.00342706, lr: 0.01, train_time: [0.31], valid_time: [0.03], best model: 0\n",
            "Epoch: 31, train_loss: 0.00267346, valid_loss: 0.00325688, lr: 0.01, train_time: [0.32], valid_time: [0.04], best model: 0\n",
            "Epoch: 32, train_loss: 0.00261435, valid_loss: 0.00320524, lr: 0.01, train_time: [0.31], valid_time: [0.03], best model: 0\n",
            "Epoch: 33, train_loss: 0.00256497, valid_loss: 0.00310586, lr: 0.01, train_time: [0.31], valid_time: [0.04], best model: 1\n",
            "Epoch: 34, train_loss: 0.00254475, valid_loss: 0.00305063, lr: 0.01, train_time: [0.32], valid_time: [0.04], best model: 1\n",
            "Epoch: 35, train_loss: 0.00254001, valid_loss: 0.00321336, lr: 0.01, train_time: [0.32], valid_time: [0.03], best model: 0\n",
            "Epoch: 36, train_loss: 0.00242199, valid_loss: 0.00300304, lr: 0.01, train_time: [0.32], valid_time: [0.04], best model: 1\n",
            "Epoch: 37, train_loss: 0.00240769, valid_loss: 0.00309819, lr: 0.01, train_time: [0.32], valid_time: [0.04], best model: 0\n",
            "Epoch: 38, train_loss: 0.0024166, valid_loss: 0.00301616, lr: 0.01, train_time: [0.33], valid_time: [0.04], best model: 0\n",
            "Epoch: 39, train_loss: 0.00243965, valid_loss: 0.00299417, lr: 0.01, train_time: [0.32], valid_time: [0.03], best model: 0\n",
            "Epoch: 40, train_loss: 0.00238894, valid_loss: 0.00309293, lr: 0.001, train_time: [0.32], valid_time: [0.04], best model: 0\n",
            "Epoch: 41, train_loss: 0.00231473, valid_loss: 0.00296711, lr: 0.001, train_time: [0.31], valid_time: [0.04], best model: 1\n",
            "Epoch: 42, train_loss: 0.00230036, valid_loss: 0.00295073, lr: 0.001, train_time: [0.32], valid_time: [0.03], best model: 1\n",
            "Epoch: 43, train_loss: 0.00228625, valid_loss: 0.00293369, lr: 0.001, train_time: [0.33], valid_time: [0.04], best model: 1\n",
            "Epoch: 44, train_loss: 0.00229913, valid_loss: 0.00294469, lr: 0.001, train_time: [0.31], valid_time: [0.04], best model: 0\n",
            "Epoch: 45, train_loss: 0.00229611, valid_loss: 0.00293748, lr: 0.001, train_time: [0.32], valid_time: [0.04], best model: 0\n",
            "Epoch: 46, train_loss: 0.00228739, valid_loss: 0.00298088, lr: 0.001, train_time: [0.31], valid_time: [0.03], best model: 0\n",
            "Epoch: 47, train_loss: 0.00228982, valid_loss: 0.0029609, lr: 0.0001, train_time: [0.31], valid_time: [0.04], best model: 0\n",
            "Early Stopped at Epoch: 48\n",
            "Tested: MAE: 3.022, RMSE : 4.758999824523926, MAPE : 12.023 %, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXTRJu0CHA6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}